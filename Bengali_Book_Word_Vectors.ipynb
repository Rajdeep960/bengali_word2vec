{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Bengali Book Word Vectors\n",
        "\n",
        "In this notebook, we will use the data we scraped from news websites to train a Word2Vec model for Bengali.\n",
        "\n",
        "Then we will test the model to see how well it is performing.\n",
        "\n",
        "First we import the packages we need"
      ],
      "metadata": {
        "id": "HKqfnYDFUfFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1enWUVRUXzv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function that will read the data file and extract the fields we want.\n",
        "\n",
        "In our case, we will be using the article body for training"
      ],
      "metadata": {
        "id": "bUzTfiiDUkUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list_all_text_of_তারাশঙ্কর বন্দ্যোপাধ্যায়.txt\n",
        "def extract_text(filename):\n",
        "\n",
        "    extracted_field=[]\n",
        "\n",
        "    with open(os.path.join('/content/drive/MyDrive/Datasets/Bengali_book_dataset', filename), 'r') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "8VoHKDBCUpQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a function to preprocess our data.\n",
        "\n",
        "The function does the following:\n",
        "- It replaces common texts found in the data and replaces that with our custom text\n",
        "- It removes all emoji's and emoticons from the text\n",
        "- It removes all English text"
      ],
      "metadata": {
        "id": "q94d-7y4UupB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_strings(texts, replace):\n",
        "    new_texts=[]\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
        "\n",
        "    for text in texts:\n",
        "        for r in replace:\n",
        "            text=text.replace(r[0], r[1])\n",
        "        text=emoji_pattern.sub(r'', text)\n",
        "        text=english_pattern.sub(r'', text)\n",
        "        text=re.sub(r'\\s+', ' ', text).strip()\n",
        "        new_texts.append(text)\n",
        "\n",
        "    return new_texts"
      ],
      "metadata": {
        "id": "-QvOXGLjUwB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to remove all the punctuations in our data. The `remove_pun` function removes all common punctuations found in text."
      ],
      "metadata": {
        "id": "AbpQ3MZUU13r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(sentences):\n",
        "    # import ipdb; ipdb.set_trace()\n",
        "    new_sentences=[]\n",
        "    exclude = list(set(string.punctuation))\n",
        "    exclude.extend([\"’\", \"‘\", \"—\"])\n",
        "    for sentence in sentences:\n",
        "        s = ''.join(ch for ch in sentence if ch not in exclude)\n",
        "        new_sentences.append(s)\n",
        "\n",
        "    return new_sentences"
      ],
      "metadata": {
        "id": "UrDZYqrFU2vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract some of the data from Ebala and print them to see how the data changes throughout the process."
      ],
      "metadata": {
        "id": "SYG3EpwQU8cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "chunksize = 10_000_000  # Set an appropriate chunk size (e.g., 10MB)\n",
        "input_file = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text.txt'\n",
        "output_file = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_without_pun.txt'\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuation_pattern = r'[.,!?;:\\-–—।‘’“”(){}\\[\\]<>«»\\'\\\"`´~]'\n",
        "    return re.sub(punctuation_pattern, '', text)\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    with open(output_file, 'a', encoding='utf-8') as outfile:\n",
        "        while True:\n",
        "            chunk = file.read(chunksize)\n",
        "            if not chunk:\n",
        "                break\n",
        "            processed_chunk = remove_punctuation(chunk)\n",
        "            outfile.write(processed_chunk)\n"
      ],
      "metadata": {
        "id": "6_kbIi38Fg7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_text = extract_text('all_text.txt')\n",
        "\n",
        "print(\"------------------Crawled Unprocessed Text-----------------------\")\n",
        "print(book_text[12])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMkeF4mU9GH",
        "outputId": "fb242b7f-5981-40a8-924c-23bc72825cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------Crawled Unprocessed Text-----------------------\n",
            "ল\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "book_text_without_punc = extract_text('all_text_without_pun.txt')\n",
        "print(\"------------------Crawled without punctuation Text-----------------------\")\n",
        "print(book_text[12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H5hT0w9Lkt4",
        "outputId": "dcfc0ce5-4e01-43ea-8251-9ee4013738cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------Crawled without pun Text-----------------------\n",
            "ল\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def replace_strings(texts, replace):\n",
        "    new_texts = []\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    english_pattern = re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
        "\n",
        "    for text in texts:\n",
        "        for r in replace:\n",
        "            text = text.replace(r[0], r[1])\n",
        "        text = emoji_pattern.sub(r'', text)\n",
        "        text = english_pattern.sub(r'', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        new_texts.append(text)\n",
        "\n",
        "    return new_texts\n",
        "\n",
        "chunksize = 10_000_000  # Set an appropriate chunk size (e.g., 10MB)\n",
        "input_file = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_without_pun.txt'\n",
        "output_file = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_file.txt'\n",
        "\n",
        "replacements=[('\\u200c', ' '),\n",
        "         ('\\u200d', ' '),\n",
        "        ('\\xa0', ' '),\n",
        "        ('\\n', ' '),\n",
        "        ('\\r', ' ')]\n",
        "\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as file:\n",
        "    with open(output_file, 'a', encoding='utf-8') as outfile:\n",
        "        while True:\n",
        "            chunk = file.read(chunksize)\n",
        "            if not chunk:\n",
        "                break\n",
        "            processed_chunk = replace_strings([chunk], replacements)\n",
        "            outfile.write(processed_chunk[0])\n"
      ],
      "metadata": {
        "id": "vn1cYrHwHt9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JWQuEpptHs2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_file.txt'\n",
        "num_words = 10  # Number of words to read\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    content = file.read()\n",
        "    words = content.split()[:num_words]\n",
        "\n",
        "for word in words:\n",
        "    print(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgzxo2Z4Htc4",
        "outputId": "11e5b79b-de34-41ad-e1e0-8eee22082614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "মাথার\n",
            "ভেতরে\n",
            "লেখা\n",
            "অদূরে\n",
            "রেস্তোরাঁ\n",
            "আষাঢ়\n",
            "সেজেছে\n",
            "খুব\n",
            "মেঘে\n",
            "মেঘেমনে\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do the same thing for the other data too"
      ],
      "metadata": {
        "id": "dS-tQ22aVGUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_text=extract_text('all_text_processed_file.txt')\n",
        "print(f\"Total Number of training data: {len(total_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkLDAM4_VN-D",
        "outputId": "6215bc7c-8b58-45b0-a1b9-8b704b58b739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of training data: 333410416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to split the articles into sentences and extract each word from those sentences.\n",
        "\n",
        "Our final training data looks like this"
      ],
      "metadata": {
        "id": "4pQvIK13VR1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filename = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_file.txt'\n",
        "output_filename = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_sentences.txt'\n",
        "chunk_size = 100_000_000  # Process 100MB at a time\n",
        "sentences = []\n",
        "sentences_to_write = 10  # Number of sentences to write\n",
        "\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    while True:\n",
        "        chunk = file.read(chunk_size)\n",
        "        if not chunk:\n",
        "            break\n",
        "        sentences.extend(re.split(r'[।!?]', chunk))\n",
        "\n",
        "with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence = sentence.strip()\n",
        "        if sentence:\n",
        "            output_file.write(sentence + '\\n')\n",
        "            sentences_to_write -= 1\n",
        "            if sentences_to_write == 0:\n",
        "                break\n"
      ],
      "metadata": {
        "id": "82wG9Ch3VSxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body=[article.split('।') for article in body]\n",
        "body=[item for sublist in body for item in sublist]\n",
        "body=[item.strip() for item in body if len(item.split())>1]\n",
        "\n",
        "body=[item.split() for item in body]\n",
        "\n",
        "print(body[:10])"
      ],
      "metadata": {
        "id": "lExu2yPeYgQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import tokenize\n",
        "\n",
        "input_filename = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_sentences.txt'\n",
        "output_filename = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_tokenized_sentences.txt'\n",
        "chunk_size = 10_000_000  # Process 10MB at a time\n",
        "\n",
        "sentences = []\n",
        "\n",
        "with open(input_filename, 'r', encoding='utf-8') as input_file:\n",
        "    with open(output_filename, 'w', encoding='utf-8') as output_file:\n",
        "        while True:\n",
        "            chunk = input_file.read(chunk_size)\n",
        "            if not chunk:\n",
        "                break\n",
        "\n",
        "            chunk_sentences = chunk.split('\\n')\n",
        "            for sentence in chunk_sentences:\n",
        "                tokenized_sentence = list(tokenize(sentence, deacc=True, lower=True))\n",
        "                sentences.append(tokenized_sentence)\n",
        "                output_file.write(' '.join(tokenized_sentence) + '\\n')\n"
      ],
      "metadata": {
        "id": "JNpsgpWxdVht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_filename = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_tokenized_sentences.txt'\n",
        "\n",
        "input_filename = '/content/drive/MyDrive/Datasets/Bengali_book_dataset/all_text_processed_sentences.txt'\n",
        "\n",
        "lines = []\n",
        "\n",
        "with open(input_filename, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "print(lines[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoFdjKnkkGsF",
        "outputId": "293156c0-16d3-4aa3-cb23-423ec03ca665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our preprocessed training data, we can start training our model.\n",
        "\n",
        "We will generate embeddings for each word of size 200 and use 5 words in its vicinity to figure out the meaning of the word"
      ],
      "metadata": {
        "id": "7ILaZeqXVXpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = gensim.models.Word2Vec(sentences=sentences, min_count=1, size=100, window=5, sg=0)"
      ],
      "metadata": {
        "id": "BCfMrUsrVW5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What are the words most similar to chele\")\n",
        "model.wv.most_similar('ছেলে', topn=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdso0JeuVbl5",
        "outputId": "09c7c9c8-da66-410b-ad00-291e398b5ce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the words most similar to chele\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('মেয়ে', 0.9104634523391724),\n",
              " ('বোন', 0.8716319799423218),\n",
              " ('ভাই', 0.8627076148986816),\n",
              " ('বাবা', 0.8575456738471985),\n",
              " ('বন্ধু', 0.8455409407615662)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What is Father + Girl - Boy =?\")\n",
        "model.wv.most_similar(positive=['', 'মেয়ে'], negative=['ছেলে'], topn=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "I9oG8YR-Vdcr",
        "outputId": "0a95895b-59f0-4108-ebac-fb73c81bd966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Father + Girl - Boy =?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7c9e1a7abde6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is Father + Girl - Boy =?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'বাবা'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'মেয়ে'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ছেলে'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'মেয়ে' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Find the odd one out')\n",
        "model.wv.doesnt_match(\"কলকাতা চেন্নাই দিল্লি রবীন্দ্রনাথ\".split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "GBbmTbywVdfe",
        "outputId": "94905a83-f8e5-4818-9ee8-77b2a322c62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Find the odd one out\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'রবীন্দ্রনাথ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"How similar are bengali and sweet?\")\n",
        "model.wv.similarity('বাঙালি', 'মিষ্টি')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfBkhDRfVhYA",
        "outputId": "d479059e-51a7-43f6-aa76-89b4851f9288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How similar are bengali and sweet?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.66019154"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.save_word2vec_format('/content/drive/MyDrive/Datasets/bengali_news_data/news_vector_text.txt', binary=False)\n",
        "model.wv.save_word2vec_format('/content/drive/MyDrive/Datasets/bengali_news_data/news_vector_binary.txt', binary=True)"
      ],
      "metadata": {
        "id": "pioS8zt9Vkvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"What about Bihari and Sweets?\")\n",
        "model.wv.similarity('বিহারি', 'মিষ্টি')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMxjlbQjVmwI",
        "outputId": "c61e5355-16a8-4d01-8eff-3e9ce10c3d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What about Bihari and Sweets?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.51406723"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}